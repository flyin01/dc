{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation with Pandas  \n",
    "1. Transforming DataFrames\n",
    "2. Aggregating DataFrames\n",
    "3. Slicing and Indexing DataFrames\n",
    "4. Creating and Visualizing DataFrames\n",
    "\n",
    "### 1. Transforming DataFrames  \n",
    "  \n",
    "**What´s the point of pandas?**  \n",
    "* Data Manipulation\n",
    " * Ch 1, Sorting and Subsetting, Creating new columns\n",
    " * Ch 2, Summary statistics, Counting, Grouped Summary statistics\n",
    " * Ch 3, Subsetting using slicing, Indexing and subsetting using index\n",
    "* Data Visualization  \n",
    " * Ch 4, Plotting, Handling missing data, Reading data into a DataFrame\n",
    "  \n",
    "  \n",
    "* **Introducing DataFrames**  \n",
    "pandas is built on top of NumPy and Matplotlib.  \n",
    "  \n",
    "Rectangular data or tabular data is the a common form of data pandas is design to work with it.  \n",
    "Each row is an observation and each column is a property or attribute. In pandas this rectangular data is represented as a `DataFrame` object. Other programming languages has something similar, `SQL` has `data table` and `R` also has `data.frame`. Every column have the same data type on each row but different columns can have different data types.  \n",
    "  \n",
    "Exploring a DataFrame object using methods/attributes:  \n",
    "  \n",
    "`df.head()` method displays the first rows (default is 5)  \n",
    "`df.indo()` method displays the names of columns, data types they contain and if there are any missing alues.  \n",
    "`df.shape` attribute shows the number of rows followed by the number of clolumns, e.g `(8,7)`  \n",
    "`df.describe()` method calculates some summary statistics for numerical columns.  \n",
    "  \n",
    "A `DataFrame` consists of three different components accessible using attributes.  \n",
    "  \n",
    "`df.values` attribute contains the data values in a 2D numpy array  \n",
    "`df.columns` attribute contains the labels of the column names as index object.  \n",
    "`df.index` attribute contains row numbers of row names as index object. Note: Row labes are stored in `.index` and not in `.rows`!  \n",
    "  \n",
    "**pandas Philosophy**  \n",
    "There should be one -- and preferably only one -- obvious way to do it!  \n",
    "*- The Zen of Python* by Tim Peters, Item 13 https://www.python.org/dev/peps/pep-0020/ \n",
    "Pandas deliberately does not follow this philosphoy, instead there are often many ways to solve a problem in pandas and we can choose the best one. pandas gives many tools to give options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspecting a DataFrame**  \n",
    "When we have a new DataFrame to work with, the first thing you need to do is explore it and see what it contains. There are several useful methods and attributes for this.  \n",
    "  \n",
    "`.head()` returns the first few rows (the “head” of the `DataFrame`).  \n",
    "`.info()` shows information on each of the columns, such as the data type and number of missing values.  \n",
    "`.shape` returns the number of rows and columns of the DataFrame.  \n",
    "`.describe()` calculates a few summary statistics for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# OMA Need to create the DF\n",
    "path = \"data/homelessness.csv\"\n",
    "homelessness = pd.read_csv(path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               region       state  individuals  family_members  state_pop\n",
      "0  East South Central     Alabama       2570.0           864.0    4887681\n",
      "1             Pacific      Alaska       1434.0           582.0     735139\n",
      "2            Mountain     Arizona       7259.0          2606.0    7158024\n",
      "3  West South Central    Arkansas       2280.0           432.0    3009733\n",
      "4             Pacific  California     109008.0         20964.0   39461588\n"
     ]
    }
   ],
   "source": [
    "# Print the head of the homelessness data\n",
    "print(homelessness.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51 entries, 0 to 50\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Unnamed: 0      51 non-null     int64  \n",
      " 1   region          51 non-null     object \n",
      " 2   state           51 non-null     object \n",
      " 3   individuals     51 non-null     float64\n",
      " 4   family_members  51 non-null     float64\n",
      " 5   state_pop       51 non-null     int64  \n",
      "dtypes: float64(2), int64(2), object(2)\n",
      "memory usage: 2.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print information about homelessness\n",
    "print(homelessness.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 6)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of homelessness\n",
    "print(homelessness.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0    individuals  family_members     state_pop\n",
      "count   51.000000      51.000000       51.000000  5.100000e+01\n",
      "mean    25.000000    7225.784314     3504.882353  6.405637e+06\n",
      "std     14.866069   15991.025083     7805.411811  7.327258e+06\n",
      "min      0.000000     434.000000       75.000000  5.776010e+05\n",
      "25%     12.500000    1446.500000      592.000000  1.777414e+06\n",
      "50%     25.000000    3082.000000     1482.000000  4.461153e+06\n",
      "75%     37.500000    6781.500000     3196.000000  7.340946e+06\n",
      "max     50.000000  109008.000000    52070.000000  3.946159e+07\n"
     ]
    }
   ],
   "source": [
    "# Print a description of homelessness\n",
    "print(homelessness.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the average number of homeless individuals of each state is about 7226.  \n",
    "Now we do some more exploration of the df!  \n",
    "  \n",
    "**Parts of a DataFrame**  \n",
    "To better understand DataFrame objects, it's useful to know that they consist of three components, stored as attributes:  \n",
    "  \n",
    "* `.values`: A two-dimensional NumPy array of values.  \n",
    "* `.columns`: An index of columns: the column names.  \n",
    "* `.index`: An index for the rows: either row numbers or row names.  \n",
    "  \n",
    "You can usually think of indexes as a list of strings or numbers, though the pandas `Index` data type allows for more sophisticated options. (These will be covered later in the course.)  \n",
    "  \n",
    "* Print a 2D NumPy array of the values in `homelessness`\n",
    "* Print the column names of `homelessness`\n",
    "* Print the index of `homelessness`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['East South Central' 'Alabama' 2570.0 864.0 4887681]\n",
      " ['Pacific' 'Alaska' 1434.0 582.0 735139]\n",
      " ['Mountain' 'Arizona' 7259.0 2606.0 7158024]\n",
      " ['West South Central' 'Arkansas' 2280.0 432.0 3009733]\n",
      " ['Pacific' 'California' 109008.0 20964.0 39461588]\n",
      " ['Mountain' 'Colorado' 7607.0 3250.0 5691287]\n",
      " ['New England' 'Connecticut' 2280.0 1696.0 3571520]\n",
      " ['South Atlantic' 'Delaware' 708.0 374.0 965479]\n",
      " ['South Atlantic' 'District of Columbia' 3770.0 3134.0 701547]\n",
      " ['South Atlantic' 'Florida' 21443.0 9587.0 21244317]\n",
      " ['South Atlantic' 'Georgia' 6943.0 2556.0 10511131]\n",
      " ['Pacific' 'Hawaii' 4131.0 2399.0 1420593]\n",
      " ['Mountain' 'Idaho' 1297.0 715.0 1750536]\n",
      " ['East North Central' 'Illinois' 6752.0 3891.0 12723071]\n",
      " ['East North Central' 'Indiana' 3776.0 1482.0 6695497]\n",
      " ['West North Central' 'Iowa' 1711.0 1038.0 3148618]\n",
      " ['West North Central' 'Kansas' 1443.0 773.0 2911359]\n",
      " ['East South Central' 'Kentucky' 2735.0 953.0 4461153]\n",
      " ['West South Central' 'Louisiana' 2540.0 519.0 4659690]\n",
      " ['New England' 'Maine' 1450.0 1066.0 1339057]\n",
      " ['South Atlantic' 'Maryland' 4914.0 2230.0 6035802]\n",
      " ['New England' 'Massachusetts' 6811.0 13257.0 6882635]\n",
      " ['East North Central' 'Michigan' 5209.0 3142.0 9984072]\n",
      " ['West North Central' 'Minnesota' 3993.0 3250.0 5606249]\n",
      " ['East South Central' 'Mississippi' 1024.0 328.0 2981020]\n",
      " ['West North Central' 'Missouri' 3776.0 2107.0 6121623]\n",
      " ['Mountain' 'Montana' 983.0 422.0 1060665]\n",
      " ['West North Central' 'Nebraska' 1745.0 676.0 1925614]\n",
      " ['Mountain' 'Nevada' 7058.0 486.0 3027341]\n",
      " ['New England' 'New Hampshire' 835.0 615.0 1353465]\n",
      " ['Mid-Atlantic' 'New Jersey' 6048.0 3350.0 8886025]\n",
      " ['Mountain' 'New Mexico' 1949.0 602.0 2092741]\n",
      " ['Mid-Atlantic' 'New York' 39827.0 52070.0 19530351]\n",
      " ['South Atlantic' 'North Carolina' 6451.0 2817.0 10381615]\n",
      " ['West North Central' 'North Dakota' 467.0 75.0 758080]\n",
      " ['East North Central' 'Ohio' 6929.0 3320.0 11676341]\n",
      " ['West South Central' 'Oklahoma' 2823.0 1048.0 3940235]\n",
      " ['Pacific' 'Oregon' 11139.0 3337.0 4181886]\n",
      " ['Mid-Atlantic' 'Pennsylvania' 8163.0 5349.0 12800922]\n",
      " ['New England' 'Rhode Island' 747.0 354.0 1058287]\n",
      " ['South Atlantic' 'South Carolina' 3082.0 851.0 5084156]\n",
      " ['West North Central' 'South Dakota' 836.0 323.0 878698]\n",
      " ['East South Central' 'Tennessee' 6139.0 1744.0 6771631]\n",
      " ['West South Central' 'Texas' 19199.0 6111.0 28628666]\n",
      " ['Mountain' 'Utah' 1904.0 972.0 3153550]\n",
      " ['New England' 'Vermont' 780.0 511.0 624358]\n",
      " ['South Atlantic' 'Virginia' 3928.0 2047.0 8501286]\n",
      " ['Pacific' 'Washington' 16424.0 5880.0 7523869]\n",
      " ['South Atlantic' 'West Virginia' 1021.0 222.0 1804291]\n",
      " ['East North Central' 'Wisconsin' 2740.0 2167.0 5807406]\n",
      " ['Mountain' 'Wyoming' 434.0 205.0 577601]]\n"
     ]
    }
   ],
   "source": [
    "# Print the values of homelessness\n",
    "print(homelessness.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['region', 'state', 'individuals', 'family_members', 'state_pop'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print the column index of homelessness\n",
    "print(homelessness.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "            34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "            50],\n",
      "           dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "# Print the row index of homelessness\n",
    "print(homelessness.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `DataFrame` has three components: values, a column index, and a row index  \n",
    "  \n",
    "**Sorting and subsetting**  \n",
    "  \n",
    "Sorting - change the order of the row by sorting them.  \n",
    "  \n",
    "`df.sort_values(\"col_name\")` a method that takes column_name as argument  \n",
    "`df.sort_values([\"col_name_1\", \"col_name_2\"], ascending = [True, False])` arguments for multiple columns and sorting order\n",
    "`df[\"col_name\"]`, to look at only one column\n",
    "`df[[\"col_name_1\"],[\"col_name_2\"]]`, the outer squared brackets are responsible for subsetting the data frame, the inner squared brackets are responsible for creating a list of column names to subset.  \n",
    "`cols_to_subset = [\"col_name_1\", \"col_name_2\"]` , create the list of columns as a variable   \n",
    "`df[cols_to_subset]`, use the separate list as a variable to subset the df  \n",
    "  \n",
    "Subsetting rows - by creating logic to filter by  \n",
    "`df[df[\"col_name_1\"] > 50]` , subset the df with a logic of a column. We can sub-set also with text and dates  \n",
    "  \n",
    "Subsetting using multiple conditions using operators:  \n",
    "`is_yes = df[\"col_name_1\"] == \"yes\"`  \n",
    "`is_no = df[\"col_name_2\"] == \"no\"`  \n",
    "`df[is_yes & is_no]`  \n",
    "  \n",
    "Or as one line of code, need paranthesis around each condition:  \n",
    "`df[ (df[\"col_name_1\"] == \"yes\") & (df[\"col_name_1\"] == \"no\") ]`  \n",
    "  \n",
    "Subsetting using categorical values using `isin()` method:  \n",
    "`is_black_or_brown = df[\"color\"].isin([\"Black\", \"Brown\"])`  \n",
    "`df[is_black_or_brown]`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sorting rows**  \n",
    "Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>state</th>\n",
       "      <th>individuals</th>\n",
       "      <th>family_members</th>\n",
       "      <th>state_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>4887681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>735139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>7259.0</td>\n",
       "      <td>2606.0</td>\n",
       "      <td>7158024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2280.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>3009733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>California</td>\n",
       "      <td>109008.0</td>\n",
       "      <td>20964.0</td>\n",
       "      <td>39461588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               region       state  individuals  family_members  state_pop\n",
       "0  East South Central     Alabama       2570.0           864.0    4887681\n",
       "1             Pacific      Alaska       1434.0           582.0     735139\n",
       "2            Mountain     Arizona       7259.0          2606.0    7158024\n",
       "3  West South Central    Arkansas       2280.0           432.0    3009733\n",
       "4             Pacific  California     109008.0         20964.0   39461588"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homelessness.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort `homelessness` by the number of homeless `individuals` in ascending order, and save this as `homelessness_ind`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                region         state  individuals  family_members  state_pop\n",
      "50            Mountain       Wyoming        434.0           205.0     577601\n",
      "34  West North Central  North Dakota        467.0            75.0     758080\n",
      "7       South Atlantic      Delaware        708.0           374.0     965479\n",
      "39         New England  Rhode Island        747.0           354.0    1058287\n",
      "45         New England       Vermont        780.0           511.0     624358\n"
     ]
    }
   ],
   "source": [
    "# Sort homelessness by individual\n",
    "homelessness_ind = homelessness.sort_values([\"individuals\"])\n",
    "\n",
    "# Print the top few rows\n",
    "print(homelessness_ind.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort `homelessness` by the number of homeless `family_members` in descending order, and save this as `homelessness_fam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                region          state  individuals  family_members  state_pop\n",
      "32        Mid-Atlantic       New York      39827.0         52070.0   19530351\n",
      "4              Pacific     California     109008.0         20964.0   39461588\n",
      "21         New England  Massachusetts       6811.0         13257.0    6882635\n",
      "9       South Atlantic        Florida      21443.0          9587.0   21244317\n",
      "43  West South Central          Texas      19199.0          6111.0   28628666\n"
     ]
    }
   ],
   "source": [
    "# Sort homelessness by descending family members\n",
    "homelessness_fam = homelessness.sort_values([\"family_members\"], ascending = False)\n",
    "\n",
    "# Print the top few rows\n",
    "print(homelessness_fam.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort `homelessness` first by region (ascending), and then by number of family members (descending). Save this as `homelessness_reg_fam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                region      state  individuals  family_members  state_pop\n",
      "13  East North Central   Illinois       6752.0          3891.0   12723071\n",
      "35  East North Central       Ohio       6929.0          3320.0   11676341\n",
      "22  East North Central   Michigan       5209.0          3142.0    9984072\n",
      "49  East North Central  Wisconsin       2740.0          2167.0    5807406\n",
      "14  East North Central    Indiana       3776.0          1482.0    6695497\n"
     ]
    }
   ],
   "source": [
    "# Sort homelessness by region, then descending family members\n",
    "homelessness_reg_fam = homelessness.sort_values([\"region\", \"family_members\"], ascending = [True, False])\n",
    "\n",
    "# Print the top few rows\n",
    "print(homelessness_reg_fam.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `.sort_values()` and `.head()` is a powerful pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subsetting columns**  \n",
    "When working with data, you may not need all of the variables in your dataset. Square brackets (`[]`) can be used to select only the columns that matter to you in an order that makes sense to you. To select only `\"col_a\"` of the DataFrame `df`, use  \n",
    "`df[\"col_a\"]`  \n",
    "To select two columns use  \n",
    "`df[[\"col_a\",\"col_b\"]]`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      2570.0\n",
      "1      1434.0\n",
      "2      7259.0\n",
      "3      2280.0\n",
      "4    109008.0\n",
      "Name: individuals, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Select the individuals column\n",
    "individuals = homelessness[\"individuals\"]\n",
    "\n",
    "# Print the head of the result\n",
    "print(individuals.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        state  family_members\n",
      "0     Alabama           864.0\n",
      "1      Alaska           582.0\n",
      "2     Arizona          2606.0\n",
      "3    Arkansas           432.0\n",
      "4  California         20964.0\n"
     ]
    }
   ],
   "source": [
    "# Select the state and family_members columns\n",
    "state_fam = homelessness[[\"state\",\"family_members\"]]\n",
    "\n",
    "# Print the head of the result\n",
    "print(state_fam.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   individuals       state\n",
      "0       2570.0     Alabama\n",
      "1       1434.0      Alaska\n",
      "2       7259.0     Arizona\n",
      "3       2280.0    Arkansas\n",
      "4     109008.0  California\n"
     ]
    }
   ],
   "source": [
    "# Select only the individuals and state columns, in that order\n",
    "ind_state = homelessness[[\"individuals\",\"state\"]]\n",
    "\n",
    "# Print the head of the result\n",
    "print(ind_state.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subsetting rows**  \n",
    "A large part of data science is about finding which bits of your dataset are interesting. One of the simplest techniques for this is to find a subset of rows that match some criteria. This is sometimes known as * filtering rows*  or *selecting rows*.  \n",
    "  \n",
    "There are many ways to subset a DataFrame, perhaps the most common is to use relational operators to return `True` or `False` for each row, then pass that inside square brackets.  \n",
    "  \n",
    "`df[df[\"height_cm\"] > 60]`  \n",
    "`df[df[\"color\"] == \"tan\"]`  \n",
    "  \n",
    "We can filter for multiple conditions at once by using the \"bitwise and\" operator, `&`.  \n",
    "  \n",
    "`df[(df[\"height_cm\"] > 60) & (df[\"color\"] == \"tan\")]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                region       state  individuals  family_members  state_pop\n",
      "4              Pacific  California     109008.0         20964.0   39461588\n",
      "9       South Atlantic     Florida      21443.0          9587.0   21244317\n",
      "32        Mid-Atlantic    New York      39827.0         52070.0   19530351\n",
      "37             Pacific      Oregon      11139.0          3337.0    4181886\n",
      "43  West South Central       Texas      19199.0          6111.0   28628666\n",
      "47             Pacific  Washington      16424.0          5880.0    7523869\n"
     ]
    }
   ],
   "source": [
    "# Filter for rows where individuals is greater than 10000\n",
    "ind_gt_10k = homelessness[homelessness[\"individuals\"] > 10000]\n",
    "\n",
    "# See the result\n",
    "print(ind_gt_10k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      region       state  individuals  family_members  state_pop\n",
      "2   Mountain     Arizona       7259.0          2606.0    7158024\n",
      "5   Mountain    Colorado       7607.0          3250.0    5691287\n",
      "12  Mountain       Idaho       1297.0           715.0    1750536\n",
      "26  Mountain     Montana        983.0           422.0    1060665\n",
      "28  Mountain      Nevada       7058.0           486.0    3027341\n",
      "31  Mountain  New Mexico       1949.0           602.0    2092741\n",
      "44  Mountain        Utah       1904.0           972.0    3153550\n",
      "50  Mountain     Wyoming        434.0           205.0     577601\n"
     ]
    }
   ],
   "source": [
    "# Filter for rows where region is Mountain\n",
    "mountain_reg = homelessness[homelessness[\"region\"] == \"Mountain\"]\n",
    "\n",
    "# See the result\n",
    "print(mountain_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    region   state  individuals  family_members  state_pop\n",
      "1  Pacific  Alaska       1434.0           582.0     735139\n"
     ]
    }
   ],
   "source": [
    "# Filter for rows where family_members is less than 1000 \n",
    "# and region is Pacific\n",
    "fam_lt_1k_pac = homelessness[ (homelessness[\"family_members\"] < 1000) & (homelessness[\"region\"]==\"Pacific\") ]\n",
    "\n",
    "# See the result\n",
    "print(fam_lt_1k_pac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using square brackets plus logical conditions is often the most powerful way of identifying interesting rows of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subsetting rows by categorical variables**  \n",
    "Subsetting data based on a categorical variable often involves using the \"or\" operator (`|`) to select rows from multiple categories. This can get tedious when you want all states in one of three different regions, for example. Instead, use the `.isin()` method, which will allow you to tackle this problem by writing one condition instead of three separate ones.  \n",
    "  \n",
    "`colors = [\"brown\", \"black\", \"tan\"]`  \n",
    "`condition = df[\"color\"].isin(colors)`  \n",
    "`df[condition]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>state</th>\n",
       "      <th>individuals</th>\n",
       "      <th>family_members</th>\n",
       "      <th>state_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>4887681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>735139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>7259.0</td>\n",
       "      <td>2606.0</td>\n",
       "      <td>7158024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2280.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>3009733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>California</td>\n",
       "      <td>109008.0</td>\n",
       "      <td>20964.0</td>\n",
       "      <td>39461588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               region       state  individuals  family_members  state_pop\n",
       "0  East South Central     Alabama       2570.0           864.0    4887681\n",
       "1             Pacific      Alaska       1434.0           582.0     735139\n",
       "2            Mountain     Arizona       7259.0          2606.0    7158024\n",
       "3  West South Central    Arkansas       2280.0           432.0    3009733\n",
       "4             Pacific  California     109008.0         20964.0   39461588"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homelessness.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            region                 state  individuals  family_members  \\\n",
      "7   South Atlantic              Delaware        708.0           374.0   \n",
      "8   South Atlantic  District of Columbia       3770.0          3134.0   \n",
      "9   South Atlantic               Florida      21443.0          9587.0   \n",
      "10  South Atlantic               Georgia       6943.0          2556.0   \n",
      "20  South Atlantic              Maryland       4914.0          2230.0   \n",
      "30    Mid-Atlantic            New Jersey       6048.0          3350.0   \n",
      "32    Mid-Atlantic              New York      39827.0         52070.0   \n",
      "33  South Atlantic        North Carolina       6451.0          2817.0   \n",
      "38    Mid-Atlantic          Pennsylvania       8163.0          5349.0   \n",
      "40  South Atlantic        South Carolina       3082.0           851.0   \n",
      "46  South Atlantic              Virginia       3928.0          2047.0   \n",
      "48  South Atlantic         West Virginia       1021.0           222.0   \n",
      "\n",
      "    state_pop  \n",
      "7      965479  \n",
      "8      701547  \n",
      "9    21244317  \n",
      "10   10511131  \n",
      "20    6035802  \n",
      "30    8886025  \n",
      "32   19530351  \n",
      "33   10381615  \n",
      "38   12800922  \n",
      "40    5084156  \n",
      "46    8501286  \n",
      "48    1804291  \n"
     ]
    }
   ],
   "source": [
    "# Subset for rows in South Atlantic or Mid-Atlantic regions\n",
    "south_mid_atlantic = homelessness[homelessness[\"region\"].isin([\"South Atlantic\",\"Mid-Atlantic\"])]\n",
    "\n",
    "# See the result\n",
    "print(south_mid_atlantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      region       state  individuals  family_members  state_pop\n",
      "2   Mountain     Arizona       7259.0          2606.0    7158024\n",
      "4    Pacific  California     109008.0         20964.0   39461588\n",
      "28  Mountain      Nevada       7058.0           486.0    3027341\n",
      "44  Mountain        Utah       1904.0           972.0    3153550\n"
     ]
    }
   ],
   "source": [
    "# The Mojave Desert states\n",
    "canu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\n",
    "\n",
    "# Filter for rows in the Mojave Desert states\n",
    "mojave_homelessness = homelessness[homelessness[\"state\"].isin(canu)]\n",
    "\n",
    "# See the result\n",
    "print(mojave_homelessness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `.isin()` makes subsetting categorical variables a breeze  \n",
    "  \n",
    "**New Columns**  \n",
    "  \n",
    "Often we want to add new columns derived from existing columns. Adding a new column can go by many names like mutating a data frame, transforming a data frame or feature engineering.  \n",
    "  \n",
    "Example of how to add a new column from existing columns:  \n",
    "`df[\"heigh_m\"] = df[\"height_cm\"]  / 100`  \n",
    "On the left hand side we have in square brackets the name of the column we want to create.  \n",
    "On the right hand side we have the calculation.  \n",
    "We will then have both the existing and the new column in the data frame.  \n",
    "  \n",
    "**Multiple manipulations**  \n",
    "Skinny dogs e.g  \n",
    "`bmi_lt_100 = df[df[\"bmi] < 100]`  \n",
    "`bmi_lt_100_height = bmi_lt_100.sort_values(\"height_cm\", ascending=False)`  \n",
    "`bmi_lt_100_height[[\"name\", \"height_cm\", \"bmi\"]]`  \n",
    "  \n",
    "Some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding new columns**  \n",
    "You aren't stuck with just the data you are given. Instead, you can add new columns to a DataFrame. This has many names, such as *transforming*, *mutating*, and *feature engineering*.  \n",
    "  \n",
    "You can create new columns from scratch, but it is also common to derive them from other columns, for example, by adding columns together or by changing their units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               region       state  individuals  family_members  state_pop  \\\n",
      "0  East South Central     Alabama       2570.0           864.0    4887681   \n",
      "1             Pacific      Alaska       1434.0           582.0     735139   \n",
      "2            Mountain     Arizona       7259.0          2606.0    7158024   \n",
      "3  West South Central    Arkansas       2280.0           432.0    3009733   \n",
      "4             Pacific  California     109008.0         20964.0   39461588   \n",
      "\n",
      "      total  p_individuals  \n",
      "0    3434.0       0.748398  \n",
      "1    2016.0       0.711310  \n",
      "2    9865.0       0.735834  \n",
      "3    2712.0       0.840708  \n",
      "4  129972.0       0.838704  \n"
     ]
    }
   ],
   "source": [
    "# Add total col as sum of individuals and family_members\n",
    "homelessness[\"total\"] = homelessness[\"individuals\"] + homelessness[\"family_members\"]\n",
    "\n",
    "# Add p_individuals col as proportion of individuals\n",
    "homelessness[\"p_individuals\"] = homelessness[\"individuals\"] / homelessness[\"total\"]\n",
    "\n",
    "# See the result\n",
    "print(homelessness.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our dataset doesn't have the exact columns you need, we can often make your own from what you have.  \n",
    "  \n",
    "**Combo-attack!**  \n",
    "You've seen the four most common types of data manipulation: sorting rows, subsetting columns, subsetting rows, and adding new columns. In a real-life data analysis, you can mix and match these four manipulations to answer a multitude of questions.  \n",
    "  \n",
    "In this exercise, you'll answer the question, \"Which state has the highest number of homeless individuals per 10,000 people in the state?\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   state  indiv_per_10k\n",
      "8   District of Columbia      53.738381\n",
      "11                Hawaii      29.079406\n",
      "4             California      27.623825\n",
      "37                Oregon      26.636307\n",
      "28                Nevada      23.314189\n",
      "47            Washington      21.829195\n",
      "32              New York      20.392363\n"
     ]
    }
   ],
   "source": [
    "# Create indiv_per_10k col as homeless individuals per 10k state pop\n",
    "homelessness[\"indiv_per_10k\"] = 10000 * homelessness[\"individuals\"] / homelessness[\"state_pop\"] \n",
    "\n",
    "# Subset rows for indiv_per_10k greater than 20\n",
    "high_homelessness = homelessness[homelessness[\"indiv_per_10k\"] > 20]\n",
    "\n",
    "# Sort high_homelessness by descending indiv_per_10k\n",
    "high_homelessness_srt = high_homelessness.sort_values(\"indiv_per_10k\", ascending=False) \n",
    "\n",
    "# From high_homelessness_srt, select the state and indiv_per_10k cols\n",
    "result = high_homelessness_srt[[\"state\",\"indiv_per_10k\"]]\n",
    "\n",
    "# See the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Washington, D.C. has the highest number of homeless individuals - almost 54 per ten thousand people. This is almost double the number of the next-highest state, Hawaii. If you combine new column addition, row subsetting, sorting, and column selection, you can answer lots of questions like this.  \n",
    "  \n",
    "### 2. Aggregating DataFrames  \n",
    "  \n",
    "**Summary statistics**  \n",
    "Focus here is on aggregating data using summary statistics. We will use dot-notation. E.g.  \n",
    "`df[\"col_1\"].mean()`  \n",
    "There are many more summary statistics that we can calculate like:  \n",
    "`.median()`  \n",
    "`.mode()`  \n",
    "`.min()`  \n",
    "`.max()`  \n",
    "`.vat()`  \n",
    "`.std()`  \n",
    "We can also take sums and calculate quantiles:  \n",
    "`.sum()`  \n",
    "`.quantile()`  \n",
    "  \n",
    "The .agg() methods  \n",
    "  \n",
    "Allows for calculating custom summary statistics.  \n",
    "`def pct30(column)`  \n",
    "` return column.quantile(0.3)`  \n",
    "Calculate the 30th percentile of df columns  \n",
    "`df[\"col_1\"].agg(pct30)` \n",
    "  \n",
    "Summaries on multiple columns  \n",
    "`df[[\"col_1\", \"col_2\"]].agg(pct30)`  \n",
    "  \n",
    "Multiple summaries  \n",
    "`def pct40(column)`  \n",
    "` return column.quantile(0.4)`  \n",
    "Calculate the 30th and 40th percentile of df column  \n",
    "`df[\"col_1\"].agg([pct30, pct40])`  \n",
    "  \n",
    "Cumulative sum  \n",
    "`df[\"col_1].cumsum()`\n",
    "It returns one sum for each row of a df.  \n",
    "Example see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>state</th>\n",
       "      <th>individuals</th>\n",
       "      <th>family_members</th>\n",
       "      <th>state_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>4887681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>735139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>7259.0</td>\n",
       "      <td>2606.0</td>\n",
       "      <td>7158024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2280.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>3009733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>California</td>\n",
       "      <td>109008.0</td>\n",
       "      <td>20964.0</td>\n",
       "      <td>39461588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               region       state  individuals  family_members  state_pop\n",
       "0  East South Central     Alabama       2570.0           864.0    4887681\n",
       "1             Pacific      Alaska       1434.0           582.0     735139\n",
       "2            Mountain     Arizona       7259.0          2606.0    7158024\n",
       "3  West South Central    Arkansas       2280.0           432.0    3009733\n",
       "4             Pacific  California     109008.0         20964.0   39461588"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# OMA Need to create the DF\n",
    "path = \"data/homelessness.csv\"\n",
    "homelessness = pd.read_csv(path, index_col=0)\n",
    "\n",
    "homelessness.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>individuals</th>\n",
       "      <th>ind_cumsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2570.0</td>\n",
       "      <td>2570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1434.0</td>\n",
       "      <td>4004.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7259.0</td>\n",
       "      <td>11263.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2280.0</td>\n",
       "      <td>13543.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>109008.0</td>\n",
       "      <td>122551.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   individuals  ind_cumsum\n",
       "0       2570.0      2570.0\n",
       "1       1434.0      4004.0\n",
       "2       7259.0     11263.0\n",
       "3       2280.0     13543.0\n",
       "4     109008.0    122551.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cumsum\n",
    "homelessness[\"ind_cumsum\"] = homelessness[\"individuals\"].cumsum()\n",
    "homelessness[[\"individuals\",\"ind_cumsum\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Cumulative Statistics  \n",
    "`.cummax()`  \n",
    "`.cummin()`  \n",
    "`.cumprod()`  \n",
    "All these return an entire column rather than one value.  \n",
    "  \n",
    "**Mean and median**  \n",
    "Some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>type</th>\n",
       "      <th>department</th>\n",
       "      <th>date</th>\n",
       "      <th>weekly_sales</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>temperature_c</th>\n",
       "      <th>fuel_price_usd_per_l</th>\n",
       "      <th>unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>False</td>\n",
       "      <td>5.727778</td>\n",
       "      <td>0.679451</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>21827.90</td>\n",
       "      <td>False</td>\n",
       "      <td>8.055556</td>\n",
       "      <td>0.693452</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-04-02</td>\n",
       "      <td>57258.43</td>\n",
       "      <td>False</td>\n",
       "      <td>16.816667</td>\n",
       "      <td>0.718284</td>\n",
       "      <td>7.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-05-07</td>\n",
       "      <td>17413.94</td>\n",
       "      <td>False</td>\n",
       "      <td>22.527778</td>\n",
       "      <td>0.748928</td>\n",
       "      <td>7.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-06-04</td>\n",
       "      <td>17558.09</td>\n",
       "      <td>False</td>\n",
       "      <td>27.050000</td>\n",
       "      <td>0.714586</td>\n",
       "      <td>7.808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store type  department        date  weekly_sales  is_holiday  \\\n",
       "0      1    A           1  2010-02-05      24924.50       False   \n",
       "1      1    A           1  2010-03-05      21827.90       False   \n",
       "2      1    A           1  2010-04-02      57258.43       False   \n",
       "3      1    A           1  2010-05-07      17413.94       False   \n",
       "4      1    A           1  2010-06-04      17558.09       False   \n",
       "\n",
       "   temperature_c  fuel_price_usd_per_l  unemployment  \n",
       "0       5.727778              0.679451         8.106  \n",
       "1       8.055556              0.693452         8.106  \n",
       "2      16.816667              0.718284         7.808  \n",
       "3      22.527778              0.748928         7.808  \n",
       "4      27.050000              0.714586         7.808  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OMA need to read in the data first\n",
    "path2 = \"data/sales_subset.csv\"\n",
    "sales = pd.read_csv(path2, index_col=0)\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   store type  department        date  weekly_sales  is_holiday  \\\n",
      "0      1    A           1  2010-02-05      24924.50       False   \n",
      "1      1    A           1  2010-03-05      21827.90       False   \n",
      "2      1    A           1  2010-04-02      57258.43       False   \n",
      "3      1    A           1  2010-05-07      17413.94       False   \n",
      "4      1    A           1  2010-06-04      17558.09       False   \n",
      "\n",
      "   temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "0       5.727778              0.679451         8.106  \n",
      "1       8.055556              0.693452         8.106  \n",
      "2      16.816667              0.718284         7.808  \n",
      "3      22.527778              0.748928         7.808  \n",
      "4      27.050000              0.714586         7.808  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10774 entries, 0 to 10773\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   store                 10774 non-null  int64  \n",
      " 1   type                  10774 non-null  object \n",
      " 2   department            10774 non-null  int64  \n",
      " 3   date                  10774 non-null  object \n",
      " 4   weekly_sales          10774 non-null  float64\n",
      " 5   is_holiday            10774 non-null  bool   \n",
      " 6   temperature_c         10774 non-null  float64\n",
      " 7   fuel_price_usd_per_l  10774 non-null  float64\n",
      " 8   unemployment          10774 non-null  float64\n",
      "dtypes: bool(1), float64(4), int64(2), object(2)\n",
      "memory usage: 768.1+ KB\n",
      "None\n",
      "23843.950148505668\n",
      "12049.064999999999\n"
     ]
    }
   ],
   "source": [
    "# Print the head of the sales DataFrame\n",
    "print(sales.head())\n",
    "\n",
    "# Print the info about the sales DataFrame\n",
    "print(sales.info())\n",
    "\n",
    "# Print the mean of weekly_sales\n",
    "print(sales[\"weekly_sales\"].mean())\n",
    "\n",
    "# Print the median of weekly_sales\n",
    "print(sales[\"weekly_sales\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean weekly sales amount is almost double the median weekly sales amount! This can tell you that there are a few very high sales weeks that are making the mean so much higher than the median.  \n",
    "  \n",
    "**Summarizing dates**  \n",
    "Summary statistics can also be calculated on date columns that have values with the data type `datetime64`. Some summary statistics — like mean — don't make a ton of sense on dates, but others are super helpful, for example, minimum and maximum, which allow you to see what time range your data covers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-10-26\n",
      "2010-02-05\n"
     ]
    }
   ],
   "source": [
    "# Print the maximum of the date column\n",
    "print(sales[\"date\"].max())\n",
    "\n",
    "# Print the minimum of the date column\n",
    "print(sales[\"date\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the minimum and maximum of a column of dates is handy for figuring out what time period your data covers. In this case, there are data from February of 2010 to October of 2012.  \n",
    "  \n",
    "**Efficient summaries**  \n",
    "While pandas and NumPy have tons of functions, sometimes, you may need a different function to summarize your data.  \n",
    "  \n",
    "The `.agg()` method allows you to apply your own custom functions to a DataFrame, as well as apply functions to more than one column of a DataFrame at once, making your aggregations super-efficient. For example,  \n",
    "`df['column'].agg(function)`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.58333333333334\n"
     ]
    }
   ],
   "source": [
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "    \n",
    "# Print IQR of the temperature_c column\n",
    "print(sales[\"temperature_c\"].agg(iqr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature_c           16.583333\n",
      "fuel_price_usd_per_l     0.073176\n",
      "unemployment             0.565000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg(iqr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        temperature_c  fuel_price_usd_per_l  unemployment\n",
      "iqr         16.583333              0.073176         0.565\n",
      "median      16.966667              0.743381         8.099\n"
     ]
    }
   ],
   "source": [
    "# Import NumPy and create custom IQR function\n",
    "import numpy as np\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr,np.median]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.agg()` method makes it easy to compute multiple statistics on multiple columns, all in just one line of code.  \n",
    "  \n",
    "**Cumulative statistics**  \n",
    "Cumulative statistics can also be helpful in tracking summary statistics over time. In this exercise, you'll calculate the cumulative sum and cumulative max of a department's weekly sales, which will allow you to identify what the total sales were so far as well as what the highest weekly sales were so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataset needed. Only include store = 1 and department = 1\n",
    "sales_1_1 = sales[ (sales[\"store\"]==1) & (sales[\"department\"]==1) ]\n",
    "sales_1_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>type</th>\n",
       "      <th>department</th>\n",
       "      <th>date</th>\n",
       "      <th>weekly_sales</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>temperature_c</th>\n",
       "      <th>fuel_price_usd_per_l</th>\n",
       "      <th>unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>False</td>\n",
       "      <td>5.727778</td>\n",
       "      <td>0.679451</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>21827.90</td>\n",
       "      <td>False</td>\n",
       "      <td>8.055556</td>\n",
       "      <td>0.693452</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-04-02</td>\n",
       "      <td>57258.43</td>\n",
       "      <td>False</td>\n",
       "      <td>16.816667</td>\n",
       "      <td>0.718284</td>\n",
       "      <td>7.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-05-07</td>\n",
       "      <td>17413.94</td>\n",
       "      <td>False</td>\n",
       "      <td>22.527778</td>\n",
       "      <td>0.748928</td>\n",
       "      <td>7.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-06-04</td>\n",
       "      <td>17558.09</td>\n",
       "      <td>False</td>\n",
       "      <td>27.050000</td>\n",
       "      <td>0.714586</td>\n",
       "      <td>7.808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store type  department        date  weekly_sales  is_holiday  \\\n",
       "0      1    A           1  2010-02-05      24924.50       False   \n",
       "1      1    A           1  2010-03-05      21827.90       False   \n",
       "2      1    A           1  2010-04-02      57258.43       False   \n",
       "3      1    A           1  2010-05-07      17413.94       False   \n",
       "4      1    A           1  2010-06-04      17558.09       False   \n",
       "\n",
       "   temperature_c  fuel_price_usd_per_l  unemployment  \n",
       "0       5.727778              0.679451         8.106  \n",
       "1       8.055556              0.693452         8.106  \n",
       "2      16.816667              0.718284         7.808  \n",
       "3      22.527778              0.748928         7.808  \n",
       "4      27.050000              0.714586         7.808  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_1_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date  weekly_sales  cum_weekly_sales  cum_max_sales\n",
      "0     2010-02-05      24924.50          24924.50       24924.50\n",
      "9009  2010-02-05      18187.71          43112.21       24924.50\n",
      "8109  2010-02-05      32313.79          75426.00       32313.79\n",
      "7199  2010-02-05      46021.21         121447.21       46021.21\n",
      "6293  2010-02-05      21500.58         142947.79       46021.21\n",
      "5408  2010-02-05      32842.31         175790.10       46021.21\n",
      "4495  2010-02-05      46761.90         222552.00       46761.90\n",
      "3593  2010-02-05      40212.84         262764.84       46761.90\n",
      "2699  2010-02-05      25619.00         288383.84       46761.90\n",
      "1798  2010-02-05      38724.42         327108.26       46761.90\n",
      "901   2010-02-05      35034.06         362142.32       46761.90\n",
      "9899  2010-02-05      21244.50         383386.82       46761.90\n"
     ]
    }
   ],
   "source": [
    "# Sort sales_1_1 by date\n",
    "sales_1_1 = sales_1_1.sort_values(\"date\").head(12)\n",
    "\n",
    "# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col\n",
    "sales_1_1[\"cum_weekly_sales\"] = sales_1_1[\"weekly_sales\"].cumsum()\n",
    "\n",
    "# Get the cumulative max of weekly_sales, add as cum_max_sales col\n",
    "sales_1_1[\"cum_max_sales\"] = sales_1_1[\"weekly_sales\"].cummax()\n",
    "\n",
    "# See the columns you calculated\n",
    "print(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all functions that calculate on columns return a single number. Some, like the cumulative statistic functions, return a whole column.  \n",
    "  \n",
    "**Counting**  \n",
    "  \n",
    "Now we will summarize categorical data using counting.  \n",
    "  \n",
    "Doprring duplicate names  \n",
    "`df.drop_duplicates(subset=\"col_name\")`  \n",
    "  \n",
    "This method takes an argument, the column_name, we want to find our duplicates based on. In this case we want all the unique names. We can drop based on two columns.  \n",
    "`df.drop_duplicates(subset=[\"col_name_1\",\"col_name_2])`  \n",
    "  \n",
    "Now we base our duplicate dropping on multiple columns by passing a list of column names into the `subset` argument. Now for example if we have string `Tom` twice in the `col_name_1` but with `Wattson` and `Johnson` in `col_name_2` we can thus keep duplicates in the `col_name_1` since it is the combination of both columns that needs to be unique.  \n",
    "  \n",
    "To count the rows in each `col_name_1` we can  \n",
    "`df[\"col_name_1\"].value_counts(sort=True)` subset the df by col_name_1 and count the rows and sort the output. We can also use the `normalize` argument to turn the counts into proportions of total.  \n",
    "`df[\"col_name_1\"].value_counts(normalize=True)`  \n",
    "  \n",
    "Some exaxmples.  \n",
    "  \n",
    "**Dropping duplicates**  \n",
    "Removing duplicates is an essential skill to get accurate counts because often, you don't want to count the same thing multiple times. In this exercise, you'll create some new DataFrames using unique values from `sales`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      store type  department        date  weekly_sales  is_holiday  \\\n",
      "0         1    A           1  2010-02-05      24924.50       False   \n",
      "3593      1    B           1  2010-02-05      40212.84       False   \n",
      "\n",
      "      temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "0          5.727778              0.679451         8.106  \n",
      "3593      12.411111              0.782478         9.765  \n",
      "    store type  department        date  weekly_sales  is_holiday  \\\n",
      "0       1    A           1  2010-02-05      24924.50       False   \n",
      "12      1    A           2  2010-02-05      50605.27       False   \n",
      "24      1    A           3  2010-02-05      13740.12       False   \n",
      "36      1    A           4  2010-02-05      39954.04       False   \n",
      "48      1    A           5  2010-02-05      32229.38       False   \n",
      "\n",
      "    temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "0        5.727778              0.679451         8.106  \n",
      "12       5.727778              0.679451         8.106  \n",
      "24       5.727778              0.679451         8.106  \n",
      "36       5.727778              0.679451         8.106  \n",
      "48       5.727778              0.679451         8.106  \n",
      "498     2010-09-10\n",
      "691     2011-11-25\n",
      "2315    2010-02-12\n",
      "6735    2012-09-07\n",
      "6810    2010-12-31\n",
      "6815    2012-02-10\n",
      "6820    2011-09-09\n",
      "Name: date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate store/type combinations\n",
    "store_types = sales.drop_duplicates(subset=[\"store\", \"type\"])\n",
    "print(store_types.head())\n",
    "\n",
    "# Drop duplicate store/department combinations\n",
    "store_depts = sales.drop_duplicates(subset=[\"store\", \"department\"])\n",
    "print(store_depts.head())\n",
    "\n",
    "# Subset the rows where is_holiday is True and drop duplicate dates\n",
    "holiday_dates = sales[sales[\"is_holiday\"]].drop_duplicates(subset=\"date\")\n",
    "\n",
    "# Print date col of holiday_dates\n",
    "print(holiday_dates[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The holiday weeks correspond to the Superbowl in February, Labor Day in September, Thanksgiving in November, and Christmas in December. Now that the duplicates are removed, it's time to do some counting.  \n",
    "  \n",
    "**Counting categorical variables**  \n",
    "Counting is a great way to get an overview of your data and to spot curiosities that you might not notice otherwise. In this exercise, you'll count the number of each type of store and the number of each department number using the DataFrames you created in the previous exercise:  \n",
    "  \n",
    "`# Drop duplicate store/type combinations`  \n",
    "`store_types = sales.drop_duplicates(subset=[\"store\", \"type\"])`  \n",
    "  \n",
    "`# Drop duplicate store/department combinations`  \n",
    "`store_depts = sales.drop_duplicates(subset=[\"store\", \"department\"])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B    1\n",
      "A    1\n",
      "Name: type, dtype: int64\n",
      "B    0.5\n",
      "A    0.5\n",
      "Name: type, dtype: float64\n",
      "99    1\n",
      "98    1\n",
      "23    1\n",
      "24    1\n",
      "25    1\n",
      "     ..\n",
      "58    1\n",
      "59    1\n",
      "60    1\n",
      "67    1\n",
      "1     1\n",
      "Name: department, Length: 80, dtype: int64\n",
      "99    0.0125\n",
      "98    0.0125\n",
      "23    0.0125\n",
      "24    0.0125\n",
      "25    0.0125\n",
      "       ...  \n",
      "58    0.0125\n",
      "59    0.0125\n",
      "60    0.0125\n",
      "67    0.0125\n",
      "1     0.0125\n",
      "Name: department, Length: 80, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of stores of each type\n",
    "store_counts = store_types[\"type\"].value_counts()\n",
    "print(store_counts)\n",
    "\n",
    "# Get the proportion of stores of each type\n",
    "store_props = store_types[\"type\"].value_counts(normalize=True)\n",
    "print(store_props)\n",
    "\n",
    "# Count the number of each department number and sort\n",
    "dept_counts_sorted = store_depts[\"department\"].value_counts()\n",
    "print(dept_counts_sorted)\n",
    "\n",
    "# Get the proportion of departments of each number and sort\n",
    "dept_props_sorted = store_depts[\"department\"].value_counts(sort=True, normalize=True)\n",
    "print(dept_props_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grouped summary statistics**  \n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://campus.datacamp.com/courses/data-manipulation-with-pandas/aggregating-dataframes?ex=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
